{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f87ec3-a532-4588-988d-f94d7682b71c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "792d67d6-e784-41a6-a860-a7e34e42a592",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "- **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that adds a penalty to the sum of the absolute values of the coefficients. This penalty encourages simpler models by shrinking some coefficients to zero, effectively performing feature selection.\n",
    "- **Difference**: Unlike Ordinary Least Squares (OLS) or Ridge Regression, Lasso can reduce the coefficients of less important features to exactly zero, thus removing them from the model. It is particularly useful when there are many irrelevant features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f6ac0-28f0-429f-bec2-07b0910d2292",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1deb527-b7a2-42f9-9879-8d5c9c4e45bc",
   "metadata": {},
   "source": [
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "- The main advantage of **Lasso Regression** is its ability to automatically select features by shrinking the coefficients of irrelevant or less important features to zero. This makes it a useful tool for **feature selection**, especially in datasets with many features, as it helps to identify and eliminate unimportant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e795df-cb23-46ac-a37b-56828c12b998",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41367889-eef6-490d-b657-1d0a9e61cd71",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "- In **Lasso Regression**, the coefficients represent the relationship between each independent variable and the dependent variable. However, due to the L1 regularization, many coefficients may be shrunk to exactly zero.\n",
    "- A coefficient of zero means the corresponding feature has been removed from the model, indicating it has no predictive power. Non-zero coefficients are interpreted in the same way as in regular linear regression: they represent the expected change in the target variable for a one-unit change in the predictor, holding other variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae219f31-2c1d-49bc-b663-e0de75016f3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ceb48b4-24b7-45a4-ad55-4419d5b73f7d",
   "metadata": {},
   "source": [
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "- The main tuning parameter in **Lasso Regression** is \\( \\lambda \\), which controls the strength of the regularization:\n",
    "  - A **higher \\( \\lambda \\)** shrinks more coefficients to zero, leading to a simpler model with fewer features.\n",
    "  - A **lower \\( \\lambda \\)** results in a model closer to ordinary linear regression, retaining more features but with a higher risk of overfitting.\n",
    "- **Cross-validation** is often used to choose the optimal \\( \\lambda \\) that balances bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbda4c7-267a-4f8c-b2f4-8c7ba51fa4c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e87e9952-be29-43ae-9a54-9a0c32c9f9e5",
   "metadata": {},
   "source": [
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "- Yes, **Lasso Regression** can be applied to non-linear problems by using **polynomial features** or other feature transformations to introduce non-linear terms into the model. Once the data is transformed, Lasso can be applied to the extended feature set. However, Lasso itself remains a linear model; it handles non-linearity through the inclusion of interaction and polynomial terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c6c03-5d82-4b85-8094-1c54baa1e510",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24b5632-e32d-4491-95ac-1096d32b3469",
   "metadata": {},
   "source": [
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "- **Ridge Regression** adds an L2 penalty (the sum of the squares of the coefficients) to the loss function, whereas **Lasso Regression** uses an L1 penalty (the sum of the absolute values of the coefficients).\n",
    "  - **Ridge** shrinks coefficients but does not set any to zero, meaning it retains all features.\n",
    "  - **Lasso** can shrink some coefficients to exactly zero, performing feature selection by eliminating less important variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eceb9c9-a005-4179-a557-b15fe3cb97ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f992f10-570e-48f6-a8f7-da0317a49572",
   "metadata": {},
   "source": [
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "- Yes, **Lasso Regression** can handle multicollinearity to some extent. In cases of multicollinearity, Lasso tends to select one of the correlated features and shrink the coefficients of the others to zero, effectively removing them from the model. This reduces the problem of multicollinearity, but Ridge Regression is often considered more robust in cases of severe multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3ba9b-1e78-4f82-b320-d1e9f99e09a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f50a9d-474d-4c20-9554-7d16a60e46b3",
   "metadata": {},
   "source": [
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "- The optimal value of \\( \\lambda \\) is typically chosen using **cross-validation**. During cross-validation, multiple values of \\( \\lambda \\) are tested, and the one that minimizes the cross-validation error (e.g., mean squared error on the validation set) is selected.\n",
    "- Additionally, methods like **Grid Search** or **Randomized Search** can be employed to efficiently explore different \\( \\lambda \\) values and identify the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e03bb-82b6-4f76-82ad-8c5d926ea95a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
